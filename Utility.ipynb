{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10949352895254782263\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1422032486\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 3290278283155374358\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mihiraman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "from Data_preprocessing import create_corpus\n",
    "from create_embeddings import get_embedding\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,labels = create_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "padded_corpus, embeddings_index, embedding_matrix, vocab_size = get_embedding(corpus,max_length=80,dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels)\n",
    "labels = keras.utils.to_categorical(labels,num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_corpus,labels,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = []\n",
    "model.append(build_model(embedding_size=50, max_words=80,y_dim=3,num_filters=200,filter_sizes = [1,2,3],dropout=0.5,vocab_size=vocab_size,embed=True, embedding_matrix=embedding_matrix, embedding_train=True))\n",
    "model.append(build_model(embedding_size=50, max_words=80,y_dim=3,num_filters=200,filter_sizes = [3,4,5],dropout=0.5,vocab_size=vocab_size,embed=True, embedding_matrix=embedding_matrix, embedding_train=True))\n",
    "model.append(build_model(embedding_size=50, max_words=80,y_dim=3,num_filters=200,filter_sizes = [5,6,7],dropout=0.5,vocab_size=vocab_size,embed=True, embedding_matrix=embedding_matrix, embedding_train=True))\n",
    "# from IPython.display import Image\n",
    "# Image(filename='shared_input_layer_testing.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    model[i].compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    batch_size = 64\n",
    "    num_epochs = 6\n",
    "    epoch_save = [3,6]\n",
    "    for j in range(num_epochs):\n",
    "        model[i].fit(X_train, y_train, validation_split=0.025, batch_size=batch_size, epochs=1, verbose=0)\n",
    "        if j+1 in epoch_save:\n",
    "            model[i].save('models/model_' + str(i+1) + '_epoch_' + str(j+1) + '.h5')\n",
    "# Timed : 00:01:56 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models from file\n",
    "def load_all_models(model_list, epoch_list):\n",
    "    all_models = []\n",
    "    for epoch_num in epoch_list:\n",
    "        for model_num in model_list: \n",
    "            filename = 'models/model_' + str(model_num) + '_epoch_' + str(epoch_num) + '.h5'\n",
    "            temp_model = load_model(filename)\n",
    "            all_models.append(temp_model)\n",
    "            print('>loaded %s' % filename)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded models/model_1_epoch_6.h5\n",
      ">loaded models/model_2_epoch_6.h5\n",
      ">loaded models/model_3_epoch_6.h5\n",
      ">loaded models/model_1_epoch_3.h5\n",
      ">loaded models/model_2_epoch_3.h5\n",
      ">loaded models/model_3_epoch_3.h5\n",
      "Loaded 6 models\n"
     ]
    }
   ],
   "source": [
    "# load models in order\n",
    "members = load_all_models([1,2,3], [6,3])\n",
    "print('Loaded %d models' % len(members))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predictions(members, X_test):\n",
    "    preds = [model.predict(X_test) for model in members]\n",
    "    preds = np.array(preds)\n",
    "    # sum across ensemble members\n",
    "    summed = np.sum(preds, axis=0)\n",
    "    # argmax across classes\n",
    "    result = np.argmax(summed, axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a specific number of members in an ensemble\n",
    "def evaluate_n_members(members, n_members, X_test, y_test):\n",
    "    subset = members[:n_members]\n",
    "    preds = ensemble_predictions(subset, X_test)\n",
    "    \n",
    "    return accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 1: single=0.750, ensemble=0.607\n",
      "> 2: single=0.756, ensemble=0.641\n",
      "> 3: single=0.754, ensemble=0.642\n",
      "> 4: single=0.748, ensemble=0.643\n",
      "> 5: single=0.737, ensemble=0.638\n",
      "> 6: single=0.741, ensemble=0.633\n"
     ]
    }
   ],
   "source": [
    "single_scores = []\n",
    "ensemble_scores = []\n",
    "y_new = np.argmax(y_test,axis=1)\n",
    "for i in range(1, len(members)+1):\n",
    "    ensemble_score = evaluate_n_members(members, i, X_test, y_new)\n",
    "    _, single_score = members[i-1].evaluate(X_test, y_test, verbose=0)\n",
    "    print('> %d: single=%.3f, ensemble=%.3f' % (i, single_score, ensemble_score))\n",
    "    ensemble_scores.append(ensemble_score)\n",
    "    single_scores.append(single_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the next commit save model with non trainable embeddings and load it again with trainable embeddings\n",
    "# weight_path=\"early_weights.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(weight_path, monitor='val_f1_score', verbose=1, save_best_only=True, mode='max')\n",
    "# early_stopping = EarlyStopping(monitor=\"val_f1_score\", mode=\"max\", patience=2)\n",
    "# callbacks = [checkpoint, early_stopping]\n",
    "# Load weights from previously trained model and retrain\n",
    "# model.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement soft voting instead of hard voting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
